{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a YOLO object detection project, data wrangling typically involves loading and preprocessing images and their corresponding annotations. Exploratory data analysis (EDA) may include visualizing images, examining the distribution of object classes, and analyzing the dimensions of bounding boxes. Here's a sample notebook in Python using Jupyter Notebook format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data Wrangling & EDA Notebook for YOLO Object Detection Project\n",
    "\n",
    "## Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "## Load data\n",
    "data_dir = 'path/to/your/dataset'\n",
    "annotations_file = 'path/to/your/annotations.txt'\n",
    "\n",
    "# Load annotations\n",
    "with open(annotations_file, 'r') as f:\n",
    "    annotations = f.readlines()\n",
    "\n",
    "## Data wrangling\n",
    "def parse_annotation(annotation):\n",
    "    parts = annotation.strip().split(' ')\n",
    "    image_path = parts[0]\n",
    "    bbox_info = [tuple(map(float, parts[i:i+5])) for i in range(1, len(parts), 5)]\n",
    "    return image_path, bbox_info\n",
    "\n",
    "# Parse annotations\n",
    "data = [parse_annotation(annotation) for annotation in annotations]\n",
    "\n",
    "# Load images and resize\n",
    "input_size = (416, 416)\n",
    "images = []\n",
    "bboxes = []\n",
    "\n",
    "for image_path, bbox_info in data:\n",
    "    image = cv2.imread(os.path.join(data_dir, image_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(image, input_size)\n",
    "    images.append(resized_image)\n",
    "    \n",
    "    bbox_info_resized = []\n",
    "    for class_id, x, y, w, h in bbox_info:\n",
    "        x, y, w, h = x * input_size[1], y * input_size[0], w * input_size[1], h * input_size[0]\n",
    "        bbox_info_resized.append((class_id, x, y, w, h))\n",
    "    bboxes.append(bbox_info_resized)\n",
    "\n",
    "## EDA\n",
    "\n",
    "# Visualize images and bounding boxes\n",
    "def visualize_image(image, bboxes):\n",
    "    image = image.copy()\n",
    "    for class_id, x, y, w, h in bboxes:\n",
    "        cv2.rectangle(image, (int(x - w/2), int(y - h/2)), (int(x + w/2), int(y + h/2)), (255, 0, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize random image\n",
    "idx = random.randint(0, len(images) - 1)\n",
    "visualize_image(images[idx], bboxes[idx])\n",
    "\n",
    "# Distribution of object classes\n",
    "class_counter = Counter()\n",
    "for bbox_info in bboxes:\n",
    "    for class_id, _, _, _, _ in bbox_info:\n",
    "        class_counter[class_id] += 1\n",
    "\n",
    "plt.bar(class_counter.keys(), class_counter.values())\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Object Classes')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of bounding box dimensions\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for bbox_info in bboxes:\n",
    "    for _, _, _, w, h in bbox_info:\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "\n",
    "plt.hist(widths, bins=20, alpha=0.5, label='Width')\n",
    "plt.hist(heights, bins=20, alpha=0.5, label='Height')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Bounding Box Dimensions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook shows how to load images and annotations, preprocess images (resize), visualize images with bounding boxes, and analyze the distribution of object classes and bounding box dimensions. Replace `'path/to/your"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
